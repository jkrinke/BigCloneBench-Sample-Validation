# Analysis of Paper 139

### Task 1: Extract Key Metadata

- **Title:** The title of the paper is not explicitly mentioned in the snippets provided. However, it is identified by its document number: 139_10.1145_3597503.3639126.
- **Authors:** The authors are Zhou et al.
- **Publication Year:** The paper is set to be presented at ICSE ’24, which indicates it is from the year 2024【4:0†source】.

### Task 2: Summarize the Paper

The paper investigates the calibration of pre-trained code models across various software engineering tasks, including functionality classification, code clone detection, defect detection, and exception type prediction. The study evaluates the effectiveness of two calibration methods, temperature scaling and label smoothing, on these models. The findings suggest that while pre-trained models achieve high accuracy, they often suffer from over-confidence, necessitating calibration for reliable deployment. The study concludes that temperature scaling is generally effective for in-distribution data, but its efficacy varies in out-of-distribution scenarios【4:8†source】【4:9†source】.

### Task 3: Extract Dataset Usage

The paper uses six datasets for evaluation, including POJ104, Java250, Python800 for functionality classification, BigCloneBench for code clone detection, Devign for defect detection, and a dataset from Kanade et al. for exception type prediction【4:0†source】【4:6†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is an empirical study focusing on model calibration rather than a literature review or survey.
  - **Quote:** Not explicitly quoted, but inferred from the context of the study's objectives and methodology【4:8†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Evaluate existing approaches.
  - **Explanation:** The paper evaluates the calibration of existing pre-trained code models.
  - **Quote:** "We evaluate the calibration of five pre-trained code models"【4:3†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is used as a benchmark for code clone detection.
  - **Quote:** "Following the work of Niu et al., we use a well-known benchmark, BigCloneBench"【4:6†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The dataset is used for evaluation, not for training.
  - **Quote:** "we use directly the dataset provided by Yang et al."【4:6†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Not explicitly mentioned.
  - **Explanation:** The paper does not specify which version of BigCloneBench is used.
  - **Quote:** Not available.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes.
  - **Explanation:** The dataset is sampled to around 10% of the CodeXGLUE benchmark.
  - **Quote:** "we use directly the dataset provided by Yang et al., which samples around 10% data from CodeXGLUE benchmark"【4:6†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not explicitly mentioned.
  - **Explanation:** The paper does not provide details on the exclusion of specific subsets like WT3/T4.
  - **Quote:** Not available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper uses a sampled version of the dataset without further modifications.
  - **Quote:** "we use directly the dataset provided by Yang et al."【4:6†source】.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** Yes.
  - **Explanation:** The dataset used is a subset created by Yang et al.
  - **Quote:** "we use directly the dataset provided by Yang et al."【4:6†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** There is no mention of manual validation or investigation of the dataset's ground truth.
  - **Quote:** Not available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could impact the validity of the clone detection results if these pairs were included in the evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** If the WT3/T4 subset was part of the evaluation, the reliability of the clone detection results could be compromised, affecting the study's conclusions about model calibration. The generalizability of the findings might be limited if the dataset's integrity is questioned. However, without explicit mention of WT3/T4's inclusion, the exact impact remains speculative.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 139, No, Evaluate existing approaches, Yes, No, Not specified, Yes, Not specified, No, Yes, No, No, Potentially
```

**Note:**  
- **E:** The version of BigCloneBench used is not specified in the paper.
- **G:** The paper does not provide details on the exclusion of specific subsets like WT3/T4.
- **L:** The finding that 93.35% of WT3/T4 pairs are not clones could potentially impact the study's results if these pairs were included in the evaluation. However, the paper does not specify whether WT3/T4 was part of the subset used.
