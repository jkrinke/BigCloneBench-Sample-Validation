# Analysis of Paper 217

### Task 1: Extract Key Metadata

- **Title:** DeepSim: Deep Learning Code Functional Similarity
- **Authors:** Gang Zhao and Jeff Huang
- **Publication Year:** 2018【4:13†source】

### Task 2: Summarize the Paper

The paper presents DeepSim, a novel approach for measuring code functional similarity using deep learning. The methodology involves encoding code control flow and data flow into a semantic matrix and using a deep neural network to learn a functional similarity metric. The study evaluates DeepSim on two datasets, including BigCloneBench, demonstrating that it significantly outperforms existing techniques in terms of recall, precision, and F1 score. The authors conclude that DeepSim effectively measures code similarity, offering improvements over syntactical approaches【4:13†source】【4:6†source】.

### Task 3: Extract Dataset Usage

The paper uses two datasets for evaluation: a dataset of 1,669 Google Code Jam projects and BigCloneBench, which contains over 6 million tagged clone pairs【4:6†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel approach rather than reviewing existing literature.
  - **Quote:** "In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix..."【4:13†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes.
  - **Explanation:** The paper introduces DeepSim, a new method for measuring code functional similarity.
  - **Quote:** "We present a novel approach for measuring code functional similarity..."【4:6†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is one of the datasets used to evaluate DeepSim.
  - **Quote:** "We evaluated DeepSim on two datasets: a dataset of 1,669 Google Code Jam projects and the popular BigCloneBench..."【4:6†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** Yes.
  - **Explanation:** The paper uses BigCloneBench as part of the training dataset for DeepSim.
  - **Quote:** "We run another experiment that use all the true/false clone pairs with functionality id 4 as training dataset..."【4:14†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The paper mentions over 6 million clone pairs, which corresponds to the old version.
  - **Quote:** "BigCloneBench is a large code clone benchmark that contains over 6,000,000 tagged clone pairs..."【4:10†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes.
  - **Explanation:** The paper mentions discarding certain code fragments and methods.
  - **Quote:** "We discard code fragments without any tagged true or false clone pairs, and discard methods with LOC less than 5."【4:10†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** No, it was included.
  - **Explanation:** WT3/T4 clone types are part of the evaluation.
  - **Quote:** "Most true/false clone pairs belong to clone type WT3/T4."【4:10†source】.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes beyond filtering.
  - **Quote:** "We discard code fragments without any tagged true or false clone pairs..."【4:10†source】.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper does not mention using a subset from previous work.
  - **Quote:** Not explicitly mentioned.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** Yes.
  - **Explanation:** The paper mentions inspecting several WT3/T4 clone pairs.
  - **Quote:** "We inspected several WT3/T4 clone pairs..."【4:14†source】.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:0†source】【4:1†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs are not clones could significantly impact the validity of the results, especially since WT3/T4 pairs were included in the evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology and conclusions relying on WT3/T4 pairs may be compromised, affecting the generalizability of the results. The high F1 score reported for WT3/T4 clones may not accurately reflect the tool's performance due to the flawed ground truth【4:12†source】【4:14†source】.

# Summary

```
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 217, No, Yes, Yes, Yes, Old, Yes, No, No, No, Yes, No, Yes
```

**Note:**  
- **F:** The ground truth of BigCloneBench was filtered by discarding code fragments without tagged true or false clone pairs and methods with LOC less than 5.
- **G:** WT3/T4 clone types were included in the evaluation, which could affect the validity of the results due to the recent finding that many WT3/T4 pairs are not actual clones.
- **L:** The finding that 93.35% of WT3/T4 pairs are not clones weakens the paper's claims, particularly those related to the evaluation of WT3/T4 clone types.
