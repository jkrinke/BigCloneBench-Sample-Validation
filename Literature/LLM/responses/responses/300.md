# Analysis of Paper 300

### Task 1: Extract Key Metadata

- **Title:** Multilingual detection of code clones using ANTLR grammar definitions
- **Authors:** W. Zhu, N. Yoshida, T. Kamiya, E. Choi, H. Takada
- **Publication Year:** 2025【4:0†source】

### Task 2: Summarize the Paper

This study aims to develop and evaluate a multilingual code clone detection technique called MSCCD, addressing the limitations of existing tools like NiCad and CCFinderSW. The methodology involves using ANTLR grammar definitions to support multiple languages and detect Type-3 clones. The study evaluates MSCCD's performance in terms of recall, precision, and scalability compared to state-of-the-art tools, using benchmarks created from the CodeNet dataset. Key findings indicate that MSCCD is a balanced detector supporting many languages, though it requires further improvements in parser performance and recall for certain clone types. The study concludes with the potential for MSCCD to enhance multilingual code clone detection practices【4:1†source】【4:4†source】.

### Task 3: Extract Dataset Usage

The paper uses the CodeNet dataset for evaluation, which includes data from competitive programming platforms like AIZU Online Judge and AtCoder. This dataset is used to create benchmarks for evaluating the precision and recall of clone detectors across multiple languages【4:1†source】【4:8†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is a research study focused on developing and evaluating a new tool, not a review or survey.
  - **Quote:** "This study aims to develop and evaluate a multilingual code clone detection technique."【4:1†source】

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes.
  - **Explanation:** The paper presents a novel approach called MSCCD.
  - **Quote:** "We address the shortcomings of NiCad and CCFinderSW and implement a multilingual syntactic code clone detector (MSCCD)."【4:1†source】

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper uses BigCloneBench to evaluate the recall of MSCCD.
  - **Quote:** "We measured the recall of MSCCD using BigCloneEval, which implements a framework of BigCloneBench."【4:11†source】

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The paper does not mention using BigCloneBench for training machine learning models.
  - **Quote:** Not applicable.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The paper refers to BigCloneBench with over 7,800,000 pairs, aligning with the old version.
  - **Quote:** "BigCloneBench contains over 7,800,000 pairs of Java clones."【4:6†source】

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes.
  - **Explanation:** WT3 and T4 results were excluded from the evaluation.
  - **Quote:** "The evaluated tools targeted syntactic code clones; hence, WT3 and T4 results were excluded from this experiment."【4:11†source】

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Yes.
  - **Explanation:** WT3 and T4 were excluded from the evaluation.
  - **Quote:** "WT3 and T4 results were excluded from this experiment."【4:11†source】

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** There is no mention of changes beyond filtering.
  - **Quote:** Not applicable.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper does not mention using a subset from previous work.
  - **Quote:** Not applicable.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any manual validation of BigCloneBench.
  - **Quote:** Not applicable.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** There is no citation of this work in the paper.
  - **Quote:** Not applicable.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes.
  - **Explanation:** The exclusion of WT3/T4 subsets, which are now known to contain a high percentage of non-clones, suggests that the paper's evaluation might not fully reflect the tool's performance on challenging clone types.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology's reliance on BigCloneBench without WT3/T4 could lead to overestimated recall and precision metrics. The conclusions about MSCCD's effectiveness might be less generalizable, especially for detecting more complex clone types. This finding highlights the need for more robust evaluation datasets and methods to ensure accurate assessments of clone detection tools.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 300, No, Yes, Yes, No, Old, Yes, Yes, No, No, No, No, Yes
```

**Note:**  
- **F:** The paper filtered out WT3 and T4 subsets from the evaluation.
- **L:** The exclusion of WT3/T4 subsets, which are now known to contain a high percentage of non-clones, suggests potential overestimation of the tool's performance.
