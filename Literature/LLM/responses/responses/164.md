# Analysis of Paper 164

### Task 1: Extract Key Metadata

- **Title:** Detecting Semantic Code Clones by Building AST-based Markov Chains Model
- **Authors:** Yueming Wu, Siyue Feng, Deqing Zou, and Hai Jin
- **Publication Year:** 2022【4:3†source】.

### Task 2: Summarize the Paper

The paper presents a novel approach called Amain for detecting semantic code clones using a tree-based method that leverages Markov chains to transform complex abstract syntax trees (ASTs) into simpler state transitions. The methodology involves generating ASTs, constructing state matrices, extracting features, and classifying code clones using machine learning techniques. The study evaluates Amain on two datasets, Google Code Jam and BigCloneBench, demonstrating superior performance over nine state-of-the-art clone detection systems. The authors conclude that Amain is effective and scalable for semantic code clone detection【4:3†source】【4:1†source】.

### Task 3: Extract Dataset Usage

The paper uses two datasets for evaluation: Google Code Jam and BigCloneBench【4:9†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel method for code clone detection rather than reviewing existing literature.
  - **Quote:** "In this paper, we propose Amain, a novel AST-based method for detecting semantic code clones"【4:1†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces Amain, a new method for detecting semantic code clones.
  - **Quote:** "In this paper, we propose Amain, a novel AST-based method for detecting semantic code clones"【4:1†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is one of the datasets used for evaluating the proposed method.
  - **Quote:** "We evaluate Amain on two widespread datasets, BigCloneBench [2, 36] and Google Code Jam [1]"【4:1†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** Yes.
  - **Explanation:** The paper uses BigCloneBench to train a machine learning classifier for code clone detection.
  - **Quote:** "After collecting the distance scores of all states, we use them to train a machine learning classifier for code clone detection"【4:2†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** New version.
  - **Explanation:** The paper mentions using more than eight million labeled clone pairs, which corresponds to the new version.
  - **Quote:** "For the second dataset, BCB [2], which consists of more than eight million labeled clone pairs"【4:9†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** The paper randomly selects a total of 270,000 clone pairs from the eight million clone pairs for training and testing.
  - **Quote:** "Since the number of non-clone code pairs in BCB is 270,000, we also randomly select a total of 270,000 clone pairs from the eight million clone pairs to perform our training and testing phase"【4:9†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** No, it was not excluded.
  - **Explanation:** The WT3/T4 subset is included in the evaluation.
  - **Quote:** "The clone pairs include 48,116 T1 pairs, 4,234 T2 pairs, 21,395 ST3 pairs, 86,341 MT3, and 109,914 WT3/T4 pairs"【4:9†source】.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes, extensions, or enrichment to the ground truth beyond filtering.
  - **Quote:** Not explicitly mentioned.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper does not mention using a subset created by previous work.
  - **Quote:** Not explicitly mentioned.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any validation or manual investigation of BigCloneBench’s ground truth.
  - **Quote:** Not explicitly mentioned.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:5†source】【4:10†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes, it potentially weakens the claims.
  - **Explanation:** The paper reports high precision and recall for detecting WT3/T4 clones, but the recent finding that 93.35% of WT3/T4 pairs are not clones suggests that the evaluation results might be overly optimistic.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology's reliance on potentially flawed ground truth could affect the validity of the results. The conclusions about the effectiveness of Amain in detecting semantic clones, particularly WT3/T4, may not be as robust as claimed. This finding could limit the generalizability of the results to real-world scenarios where the ground truth is more accurate.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 164, No, Yes, Yes, Yes, New, Yes, No, No, No, No, No, Potentially
```

**Note:**  
- **F:** The ground truth of BigCloneBench was filtered to select 270,000 clone pairs for training and testing.
- **G:** The WT3/T4 subset was included in the evaluation.
- **L:** The recent finding that 93.35% of WT3/T4 pairs are not clones potentially weakens the claims regarding the effectiveness of the method in detecting WT3/T4 clones.
