# Analysis of Paper 361

### Task 1: Extract Key Metadata

- **Title:** Code Clone Detection with Hierarchical Attentive Graph Embedding
- **Authors:** Xiujuan Ji, Lei Liu, Jingwen Zhu
- **Publication Year:** 2021【4:2†source】

### Task 2: Summarize the Paper

The paper presents a novel approach to code clone detection using a Hierarchical Attentive Graph Neural Network (HAG). The objective is to improve the detection of code clones by leveraging the hierarchical structure of Abstract Syntax Trees (AST) and employing attention mechanisms to focus on critical nodes. The methodology involves transforming source code into ASTs, using Word2vec for node vector generation, and applying graph convolutional networks for feature learning. The study evaluates HAG against traditional and state-of-the-art clone detection tools using the BigCloneBench dataset, demonstrating superior precision and recall. The paper concludes that HAG effectively captures semantic similarities in code, particularly for Type-3 and Type-4 clones【4:0†source】【4:3†source】.

### Task 3: Extract Dataset Usage

The paper uses the BigCloneBench dataset for evaluation, which is a real-world benchmark with manually validated clones. The dataset includes 8,584,153 true clone pairs and 279,032 false clone pairs, covering 10 types of functionalities【4:13†source】【4:16†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel approach rather than reviewing existing literature systematically.
  - **Quote:** "In this paper, we adopt a hierarchical attentive graph (HAG) neural network for the fine-grained code embedding"【4:3†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces a new method called HAG for code clone detection.
  - **Quote:** "We propose a hierarchical attentive graph neural network embedding model-HAG for the code clone detection"【4:2†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper evaluates the HAG model using the BigCloneBench dataset.
  - **Quote:** "The experiments are conducted on BigCloneBench, a real-world benchmark with manually validated clones"【4:16†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** Yes.
  - **Explanation:** The dataset is used to train and test the HAG model.
  - **Quote:** "We randomly divide it into two parts, of which the proportions are 80%, 20% for training and testing"【4:16†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The dataset covers 10 types of functionalities, consistent with the old version.
  - **Quote:** "The dataset covers 10 types of functionalities including 8,584,153 true clone pairs"【4:16†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** The paper discards code fragments without any tagged pairs and those with lines less than 6.
  - **Quote:** "We select part of the code fragments which discards code fragments without any tagged pairs and the source code fragments with lines less than 6"【4:13†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** No, it was included.
  - **Explanation:** The paper includes WT3/4 in its evaluation.
  - **Quote:** "The fine-grained comparison for clone types is shown in Table 3"【4:15†source】.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes beyond filtering.
  - **Quote:** Not explicitly mentioned.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper does not mention using a subset from previous work.
  - **Quote:** Not explicitly mentioned.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any manual validation of the dataset.
  - **Quote:** Not explicitly mentioned.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:10†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes, it potentially weakens the claims.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs are not clones could significantly affect the validity of the results, especially since the paper evaluates performance on WT3/4 clones.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology's reliance on potentially incorrect ground truth for WT3/4 clones could lead to overestimated performance metrics. This affects the generalizability of the conclusions regarding the model's effectiveness in detecting semantic clones【4:15†source】.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 361, No, Yes, Yes, Yes, Old, Yes, No, No, No, No, No, Potentially
```

**Note:**  
- **F:** The dataset was filtered to exclude code fragments without tagged pairs and those with fewer than 6 lines.
- **L:** The finding that 93.35% of WT3/T4 pairs are not clones potentially weakens the paper's claims, particularly regarding the evaluation of WT3/4 clones.
