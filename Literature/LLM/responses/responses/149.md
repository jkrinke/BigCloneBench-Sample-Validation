# Analysis of Paper 149

### Task 1: Extract Key Metadata

- **Title:** On Inter-Dataset Code Duplication and Data Leakage in Large Language Models
- **Authors:** José Antonio Hernández López, Boqi Chen, Mootez Saad, Tushar Sharma, and Dániel Varró
- **Publication Year:** 2025【4:5†source】.

### Task 2: Summarize the Paper

The paper investigates the phenomenon of inter-dataset code duplication and its impact on the evaluation of large language models (LLMs) in software engineering tasks. The study employs an empirical approach using the CODESEARCHNET dataset and five fine-tuning datasets to identify overlaps that could lead to inflated performance metrics. The authors pre-train two versions of LLMs—one with and one without the identified overlaps—and compare their performances. The findings reveal a significant threat to the validity of LLM evaluations due to inter-dataset code duplication, particularly when using lightweight fine-tuning techniques. The paper concludes by emphasizing the need for careful dataset composition to ensure robust evaluation methodologies【4:5†source】【4:6†source】.

### Task 3: Extract Dataset Usage

The paper uses several datasets for evaluation, including CODESEARCHNET for pre-training and five fine-tuning datasets: CODETRANS, PYTHON-150, TL-CODESUM, FUNCOM, and BIGCLONEBENCH. The study specifically uses a filtered version of BigCloneBench available in CODEXGLUE, which includes around 9,000 code snippets【4:1†source】【4:16†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is an empirical study focusing on inter-dataset code duplication and its impact on LLM evaluations.
  - **Quote:** "This paper explores the phenomenon of inter-dataset code duplication and its impact on evaluating LLMs across diverse SE tasks"【4:5†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** No.
  - **Explanation:** The paper evaluates the impact of inter-dataset code duplication on LLM evaluations rather than presenting a new clone detection approach.
  - **Quote:** "We conduct an empirical study using the CODESEARCHNET dataset (CSN), a widely adopted pre-training dataset, and five fine-tuning datasets used for various SE tasks"【4:5†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper uses a filtered version of BigCloneBench as part of its evaluation datasets.
  - **Quote:** "The BIGCLONEBENCH dataset [22] is a comprehensive dataset designed for code clone detection... We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The paper uses BigCloneBench for evaluation, not for training.
  - **Quote:** "We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** New version.
  - **Explanation:** The paper uses a filtered version from CODEXGLUE, which aligns with the new version's characteristics.
  - **Quote:** "We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes.
  - **Explanation:** The paper uses a filtered version of BigCloneBench.
  - **Quote:** "We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not specified.
  - **Explanation:** The paper does not explicitly mention the exclusion or inclusion of the WT3/T4 subset.
  - **Quote:** Not available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper uses a filtered version but does not mention any other modifications.
  - **Quote:** "We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** Yes.
  - **Explanation:** The paper uses the CODEXGLUE benchmark, which is a subset created by previous work.
  - **Quote:** "We use the filtered version available in CODEXGLUE [9], including around 9k code snippets"【4:16†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any validation or manual investigation of BigCloneBench’s ground truth.
  - **Quote:** Not available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially.
  - **Explanation:** The recent finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could impact the validity of evaluations using this dataset. If the paper relies on these pairs, the results might be compromised.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology might need revision to exclude unreliable subsets, and conclusions drawn from evaluations using these pairs could be less reliable. The generalizability of the findings might be affected if the dataset's integrity is compromised.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 149, No, No, Yes, No, New, Yes, Not specified, No, Yes, No, No, Potentially
```

**Note:**  
- **G:** The paper does not specify whether the WT3/T4 subset was excluded or included in the evaluation.
- **L:** The recent finding about the WT3/T4 pairs potentially impacts the validity of the paper's results if these pairs were used in the evaluation.
