# Analysis of Paper 188

### Task 1: Extract Key Metadata

- **Title:** Greening Large Language Models of Code
- **Authors:** Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo
- **Publication Year:** 2024【4:7†source】

### Task 2: Summarize the Paper

The paper titled "Greening Large Language Models of Code" introduces Avatar, a novel approach designed to optimize large language models of code for deployment on developers' devices. The primary objective is to reduce model size, inference latency, energy consumption, and carbon footprint while maintaining effectiveness. Avatar formulates the optimization as a multi-objective configuration tuning problem, solved using a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The study evaluates Avatar using CodeBERT and GraphCodeBERT on tasks like vulnerability prediction and clone detection, demonstrating significant improvements in efficiency metrics with minimal loss in effectiveness【4:7†source】【4:8†source】.

### Task 3: Extract Dataset Usage

The paper uses two datasets for evaluation: Devign for vulnerability prediction and BigCloneBench for clone detection. BigCloneBench is used to evaluate Avatar's effectiveness in clone detection, with a subset of 90,102 examples selected for training and 4,000 for validation and testing【4:1†source】【4:12†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel approach rather than reviewing existing literature.
  - **Quote:** "This paper proposes Avatar, a novel approach that can optimize large language models of code..."【4:8†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces Avatar, a new method for optimizing language models.
  - **Quote:** "To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code..."【4:7†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is used to evaluate Avatar's effectiveness in clone detection.
  - **Quote:** "For evaluating Avatar’s effectiveness in clone detection, we select the widely-used BigCloneBench dataset..."【4:1†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** Yes.
  - **Explanation:** The dataset is used for training and evaluating the models.
  - **Quote:** "We follow recent studies [65, 79] to randomly select 90,102 examples (i.e., 10% of the original training dataset) for training..."【4:1†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The dataset description matches the old version with 6 million clone pairs.
  - **Quote:** "It includes over 6,000,000 pairs of cloned Java methods..."【4:1†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** A subset of 90,102 examples was selected for training.
  - **Quote:** "We follow recent studies [65, 79] to randomly select 90,102 examples..."【4:1†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not explicitly mentioned.
  - **Explanation:** The paper does not specify whether WT3/T4 was included or excluded.
  - **Quote:** No specific quote available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes beyond filtering.
  - **Quote:** No specific quote available.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** Yes.
  - **Explanation:** The paper follows previous studies to select the subset.
  - **Quote:** "We follow recent studies [65, 79] to randomly select 90,102 examples..."【4:1†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** There is no mention of manual validation or investigation.
  - **Quote:** No specific quote available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** No specific quote available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially, yes.
  - **Explanation:** If 93.35% of WT3/T4 pairs are not clones, this could affect the validity of the clone detection results if these pairs were included in the evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology might be compromised if the dataset's integrity is questioned. Conclusions drawn from potentially flawed data could be less reliable, affecting the generalizability of the findings. The paper's claims about Avatar's effectiveness in clone detection might need reevaluation if the dataset's ground truth is inaccurate.

# Summary

```
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 188, No, Yes, Yes, Yes, Old, Yes, Not specified, No, Yes, No, No, Potentially
```

**Note:**  
- For question G, the paper does not specify whether the WT3/T4 subset was included or excluded.
- For question L, the term "Potentially" is used because the new findings about WT3/T4 could affect the validity of the paper's results if these pairs were included in the evaluation.
