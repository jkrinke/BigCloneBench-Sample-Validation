# Analysis of Paper 204

### Task 1: Extract Key Metadata

- **Title:** Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond
- **Authors:** Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, Hongbin Sun
- **Publication Year:** 2023【4:14†source】.

### Task 2: Summarize the Paper

The paper titled "Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond" explores the efficiency of fine-tuning pre-trained code models for software engineering tasks. The authors investigate the layer-wise representations of pre-trained models and propose efficient alternatives to traditional fine-tuning by freezing certain layers. They conduct extensive experiments on five downstream tasks, including clone detection, using datasets like BigCloneBench. The study finds that freezing layers can significantly reduce computational costs while maintaining or improving performance, offering a more efficient approach to adapting pre-trained models for specific tasks【4:14†source】【4:19†source】.

### Task 3: Extract Dataset Usage

The paper uses several datasets for evaluation, including BigCloneBench for clone detection, CodeSearchNet for code search and summarization, CONCODE for code generation, and the GitHub Java Corpus for code completion【4:8†source】【4:19†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is an empirical study focusing on fine-tuning pre-trained code models rather than reviewing existing literature.
  - **Quote:** "In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning"【4:14†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it evaluates existing approaches.
  - **Explanation:** The paper evaluates the performance of pre-trained models on clone detection tasks.
  - **Quote:** "We conduct experiments on five diverse downstream tasks including code search, clone detection, code summarization, code generation, and line-level code completion"【4:6†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is used as a dataset for evaluating clone detection.
  - **Quote:** "For Clone detection, we experiment on the commonly-used BigCloneBench dataset"【4:19†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The paper uses BigCloneBench for evaluation, not for training.
  - **Quote:** "For Clone detection, we experiment on the commonly-used BigCloneBench dataset and use the precision (P), recall (R), and F1-score (F1) as evaluation metrics"【4:19†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** The version is not explicitly mentioned.
  - **Explanation:** The paper does not specify which version of BigCloneBench is used.
  - **Quote:** Not available.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** No information provided.
  - **Explanation:** The paper does not mention any filtering or modification of BigCloneBench.
  - **Quote:** Not available.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** No information provided.
  - **Explanation:** The paper does not mention the exclusion of the WT3/T4 subset.
  - **Quote:** Not available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No information provided.
  - **Explanation:** The paper does not mention any changes to the ground truth of BigCloneBench.
  - **Quote:** Not available.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No information provided.
  - **Explanation:** The paper does not mention using a subset created by previous work.
  - **Quote:** Not available.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any validation or manual investigation of BigCloneBench’s ground truth.
  - **Quote:** Not available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could impact the validity of the clone detection results if these pairs were included in the evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** If the WT3/T4 subset was used, the precision, recall, and F1-score metrics reported in the paper might be inflated, affecting the conclusions about the efficiency and effectiveness of the fine-tuning approach. This could limit the generalizability of the findings to real-world clone detection tasks.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 204, No, Yes, Yes, No, Not specified, No, No, No, No, No, No, Potentially
```

**Note:**  
- The version of BigCloneBench used in the paper is not specified, hence "Not specified" is used for question E.
- The potential impact of the WT3/T4 finding on the paper's results is acknowledged with "Potentially" for question L, as the paper does not specify whether this subset was used.
