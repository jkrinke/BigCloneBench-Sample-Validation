# Analysis of Paper 179

### Task 1: Extract Key Metadata

- **Title:** CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code
- **Authors:** Aryaz Eghbali and Michael Pradel
- **Publication Year:** 2022【4:0†source】

### Task 2: Summarize the Paper

The paper introduces CrystalBLEU, a metric designed to evaluate code similarity by addressing the limitations of the traditional BLEU score, which is affected by trivially shared n-grams. The authors propose a method to remove these n-grams to improve the metric's ability to distinguish between semantically similar and dissimilar code. The study evaluates CrystalBLEU using datasets like BigCloneBench and ShareCode, demonstrating its superior distinguishability compared to BLEU and CodeBLEU. The findings suggest that CrystalBLEU is more effective for code similarity tasks, offering a more precise metric without significant computational overhead【4:3†source】【4:5†source】.

### Task 3: Extract Dataset Usage

The paper uses three datasets for evaluation: BigCloneBench, ShareCode, and Concode. BigCloneBench is used to evaluate clone and non-clone pairs, ShareCode provides semantically equivalent code snippets, and Concode is used for code generation tasks【4:5†source】【4:19†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel metric for code similarity evaluation rather than reviewing existing literature.
  - **Quote:** "This paper presents CrystalBLEU, a metric to precisely and efficiently evaluate code similarity despite trivially shared n-grams"【4:3†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces CrystalBLEU, a new metric for evaluating code similarity.
  - **Quote:** "This paper presents CrystalBLEU, a metric to precisely and efficiently evaluate code similarity"【4:3†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is used to evaluate the effectiveness of CrystalBLEU in distinguishing clone and non-clone pairs.
  - **Quote:** "We evaluate our work with three datasets... Another dataset is BigCloneBench"【4:3†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The dataset is used for evaluation, not for training a machine learning model.
  - **Quote:** "We perform these experiments on the two datasets that provide equivalent and non-equivalent code pairs, i.e., ShareCode and BigCloneBench"【4:5†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The paper mentions using more than 1.7 million pairs, which aligns with the old version.
  - **Quote:** "BigCloneBench: Clone and non-clone pairs. This dataset contains more than 1.7 million inter-project pairs of Java programs"【4:5†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** No filtering or modification is mentioned.
  - **Explanation:** The paper does not specify any filtering or modification of the BigCloneBench dataset.
  - **Quote:** Not explicitly mentioned in the text.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not mentioned.
  - **Explanation:** The paper does not discuss the exclusion of any specific subsets like WT3/T4.
  - **Quote:** Not explicitly mentioned in the text.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper uses the dataset as is for evaluation purposes.
  - **Quote:** Not explicitly mentioned in the text.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper uses the full dataset as available in the CodeXGLUE repository.
  - **Quote:** "It is available on the CodeXGLUE repository"【4:5†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any manual validation of the dataset.
  - **Quote:** Not explicitly mentioned in the text.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:0†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially weakens.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs are not clones could affect the validity of using BigCloneBench as a reliable ground truth for evaluating code similarity metrics.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology relies on the assumption that BigCloneBench provides accurate clone and non-clone labels. If a significant portion of the dataset is mislabeled, it could lead to misleading conclusions about the effectiveness of CrystalBLEU. The generalizability of the results might be compromised if the dataset's integrity is in question.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 179, No, Yes, Yes, No, Old, No, Not specified, No, No, No, No, Potentially
```

**Note:**  
- **G:** The paper does not specify whether the WT3/T4 subset was excluded from evaluation.
- **L:** The finding that 93.35% of WT3/T4 pairs are not clones potentially weakens the paper's claims, as it relies on BigCloneBench for evaluation.
