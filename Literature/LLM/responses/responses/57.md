# Analysis of Paper 57

### Task 1: Extract Key Metadata

- **Title:** Lancer: Your Code Tell Me What You Need
- **Authors:** Shufan Zhou, Beijun Shen, Hao Zhong
- **Publication Year:** 2019【4:10†source】.

### Task 2: Summarize the Paper

The paper presents Lancer, a context-aware code-to-code recommendation tool designed to assist programmers by suggesting relevant code samples in real-time. Lancer leverages a Library-Sensitive Language Model (LSLM) and a BERT model to analyze the intention behind incomplete code and recommend suitable code snippets. The methodology involves constructing programming tasks and evaluating Lancer against other tools like SLAMPA, SourcererCC, and CCLearner. Key findings indicate that Lancer outperforms these tools in terms of recommendation accuracy and speed. The study concludes that Lancer significantly enhances code recommendation efficiency, with plans for future improvements【4:10†source】【4:1†source】.

### Task 3: Extract Dataset Usage

The paper uses the BigCloneBench dataset for evaluation, specifically selecting 42,120 source files that include 79,563 clones. The dataset is divided into ten categories based on functionalities, and the clones are manually marked as T1, T2, ST3, MT3, and WT3/4【4:6†source】【4:11†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel tool rather than reviewing existing literature.
  - **Quote:** "In this paper, we present Lancer, a context-aware code-to-code recommending tool..."【4:1†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces Lancer, a new tool for code recommendation.
  - **Quote:** "In this paper, we present Lancer, a context-aware code-to-code recommending tool..."【4:1†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper uses BigCloneBench as part of its evaluation dataset.
  - **Quote:** "We select the BigCloneBench benchmark as the dataset of our evaluation."【4:6†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** Yes.
  - **Explanation:** The clone relations in BigCloneBench are used as labels for training the semantic ranking model.
  - **Quote:** "Our semantic ranking model needs labels, and we use the clone relations in the training set as the labels."【4:6†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The paper mentions 10 categories, which aligns with the old version of BigCloneBench.
  - **Quote:** "The benchmark has 42,120 source files, which includes 301,537 methods, and 8 million LOCs in total. According to their functionalities, the source code files in this benchmark are divided into ten categories..."【4:6†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** The paper selects specific types of clones (T1, T2, ST3, MT3) for evaluation.
  - **Quote:** "From BigCloneBench, we select all the T1, T2, ST3, and MT3 clones, which include 42,120 files with 79,563 clones."【4:6†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Yes, WT3/T4 was excluded.
  - **Explanation:** The paper explicitly states the exclusion of WT3/T4 clones.
  - **Quote:** "From BigCloneBench, we select all the T1, T2, ST3, and MT3 clones..."【4:6†source】.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes beyond filtering.
  - **Quote:** Not explicitly stated, but inferred from the dataset usage description【4:6†source】.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** Yes.
  - **Explanation:** The paper follows a similar approach to Li et al. for dataset preparation.
  - **Quote:** "When training our models, as Li et al. did, we select the 23,193 source files under the 'copy file' category as our training set..."【4:6†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** There is no mention of manual validation or investigation of the dataset.
  - **Quote:** Not explicitly stated, but inferred from the methodology【4:6†source】.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:0†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes, it potentially weakens the claims.
  - **Explanation:** The exclusion of WT3/T4 clones, which are now known to be largely incorrect, suggests that the paper's evaluation might not fully represent the dataset's reliability.
  - **Impact:** The methodology and conclusions might be affected as the dataset's integrity is compromised. The generalizability of the results could be questioned, as the evaluation did not consider the flawed WT3/T4 subset, which could have influenced the tool's performance metrics.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 57, No, Yes, Yes, Yes, Old, Yes, Yes, No, Yes, No, No, Potentially
```

**Note:**  
- **A:** The paper is not a systematic literature review or a survey.
- **B:** The paper presents a novel clone detection approach.
- **C:** BigCloneBench is used for evaluation.
- **D:** BigCloneBench is used as ground truth for training.
- **E:** The old version of BigCloneBench is used.
- **F:** The dataset was filtered to exclude WT3/T4 clones.
- **G:** WT3/T4 subset was excluded from evaluation.
- **H:** The ground truth was not changed beyond filtering.
- **I:** A subset created by previous work was used.
- **J:** The paper did not validate or manually investigate BigCloneBench’s ground truth.
- **K:** The paper does not cite "BigCloneBench Considered Harmful for Machine Learning."
- **L:** The recent finding potentially weakens the paper's claims due to the exclusion of WT3/T4 clones.
