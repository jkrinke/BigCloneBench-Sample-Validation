# Analysis of Paper 156

### Task 1: Extract Key Metadata

- **Title:** Generalizability of Code Clone Detection on CodeBERT
- **Authors:** Tim Sonnekalb, Bernd Gruner, Clemens-Alexander Brust, Patrick Mäder
- **Publication Year:** 2022【4:1†source】

### Task 2: Summarize the Paper

The paper titled "Generalizability of Code Clone Detection on CodeBERT" investigates the performance and generalizability of the CodeBERT model for code clone detection. The study evaluates CodeBERT using subsets of the BigCloneBench dataset, focusing on its ability to generalize across different code snippets and functionalities. The methodology involves creating two evaluation datasets by filtering BigCloneBench and assessing the model's performance on these subsets. Key findings indicate a significant drop in F1 score when CodeBERT is evaluated on unseen data, suggesting limitations in its generalizability. The paper concludes with recommendations for improving benchmark datasets and highlights the challenges posed by dataset imbalances【4:1†source】【4:3†source】【4:7†source】.

### Task 3: Extract Dataset Usage

The paper uses the BigCloneBench dataset for evaluation, specifically focusing on subsets created from the BCB v2 version. It also references the CodeXGlue benchmark, which includes a pre-processed version of BigCloneBench【4:6†source】【4:8†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is an empirical study evaluating the generalizability of CodeBERT for code clone detection.
  - **Quote:** "We investigated the generalizability of the CodeBERT model by evaluating on unseen data from a larger portion of the same dataset." (Page 3)【4:3†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Evaluate existing approaches.
  - **Explanation:** The paper evaluates the existing CodeBERT model's performance on code clone detection.
  - **Quote:** "We show that the generalizability of CodeBERT decreases by evaluating two different subsets of Java code clones from BigCloneBench." (Page 1)【4:1†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper uses subsets of BigCloneBench to evaluate CodeBERT's performance.
  - **Quote:** "We evaluate two different subsets of Java code clones from BigCloneBench." (Page 1)【4:1†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The paper uses BigCloneBench for evaluation, not for training.
  - **Quote:** "We recommend the authors of CodeXGlue to revise their benchmark and use a more extensive training dataset by evaluating on the whole BigCloneBench dataset." (Page 3)【4:3†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** New version.
  - **Explanation:** The paper uses BCB v2, which is the new version of BigCloneBench.
  - **Quote:** "In a follow-up work [9], the authors mine a larger version of BigCloneBench (BCB v2) with 43 code functionalities." (Page 2)【4:6†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** The paper creates two evaluation sets by filtering BCB v2.
  - **Quote:** "We create two evaluation sets by filtering BCBv2." (Page 2)【4:9†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not explicitly mentioned.
  - **Explanation:** The paper does not specify the exclusion of WT3/T4 subsets.
  - **Quote:** Not available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper uses the existing dataset without further changes beyond filtering.
  - **Quote:** "We use the same hyperparameters and base model codebert-base as the benchmark to ensure comparability." (Page 3)【4:9†source】.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** Yes.
  - **Explanation:** The paper uses subsets of BCB v2, which is a version of BigCloneBench.
  - **Quote:** "We create two evaluation sets by filtering BCBv2." (Page 2)【4:9†source】.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention manual validation of BigCloneBench’s ground truth.
  - **Quote:** Not available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could significantly impact the paper's evaluation results, as it questions the reliability of the dataset used for evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology relying on BigCloneBench's ground truth may be flawed, affecting the conclusions about CodeBERT's generalizability. The generalizability claims may not hold if the dataset's ground truth is inaccurate, leading to potential overestimation of CodeBERT's performance. This finding suggests a need for re-evaluation using a more reliable dataset.

# Summary

```plaintext
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 156, No, Evaluate existing, Yes, No, New, Yes, Not specified, No, Yes, No, No, Yes
```

**Note:**  
- **G:** The paper does not specify whether the WT3/T4 subset was excluded from evaluation.
- **L:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones potentially weakens the paper's claims regarding the generalizability of CodeBERT, as it questions the reliability of the dataset used for evaluation.
