# Analysis of Paper 219

### Task 1: Extract Key Metadata

- **Title:** Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations
- **Authors:** Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang
- **Publication Year:** 2021【4:18†source】

### Task 2: Summarize the Paper

The paper introduces Corder, a self-supervised contrastive learning framework designed to improve code retrieval and summarization tasks without the need for labeled data. The authors leverage semantic-preserving transformations to generate diverse yet semantically equivalent code snippets, which are used to train a model to distinguish between similar and dissimilar code. The study evaluates the effectiveness of Corder in producing vector representations of code for retrieval tasks and in fine-tuning for tasks requiring labeled data, such as code summarization. The results demonstrate that Corder significantly outperforms existing baselines across multiple tasks【4:18†source】.

### Task 3: Extract Dataset Usage

The paper uses the BigCloneBench (BCB) dataset and the OJ dataset for evaluation. The BCB dataset is used for code clone detection tasks, which involves detecting semantically duplicated or similar code snippets【4:0†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper presents a novel framework and evaluates its performance, rather than reviewing existing literature.
  - **Quote:** "We propose Corder, a self-supervised contrastive learning framework for source code model."【4:18†source】

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Yes, it presents a novel approach.
  - **Explanation:** The paper introduces Corder, a new framework for code retrieval and summarization.
  - **Quote:** "We propose Corder, a self-supervised contrastive learning framework for source code model."【4:18†source】

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** The paper uses BigCloneBench for evaluating code clone detection tasks.
  - **Quote:** "BigCloneBench (BCB) dataset [65] contains 25,000 Java projects, cover 10 functionalities and including 6,000,000 true clone pairs and 260,000 false clone pairs."【4:0†source】

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The dataset is used for evaluation, not for training.
  - **Quote:** "The datasets we used to evaluate for this task are... BigCloneBench (BCB) dataset."【4:0†source】

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** Old version.
  - **Explanation:** The paper mentions 6,000,000 true clone pairs and 260,000 false clone pairs, which corresponds to the old version.
  - **Quote:** "BigCloneBench (BCB) dataset [65] contains... 6,000,000 true clone pairs and 260,000 false clone pairs."【4:0†source】

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, it has been filtered.
  - **Explanation:** The dataset size was reduced by selecting 50,000 sample clone pairs and 50,000 non-clone pairs.
  - **Quote:** "We reduce the size by randomly select 50,000 sample clone pairs and 50,000 samples none clone pairs."【4:2†source】

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** Not specified.
  - **Explanation:** The paper does not mention specific subsets like WT3/T4.
  - **Quote:** Not available.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not indicate any changes beyond filtering.
  - **Quote:** Not available.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper describes its own method of filtering the dataset.
  - **Quote:** "We reduce the size by randomly select 50,000 sample clone pairs and 50,000 samples none clone pairs."【4:2†source】

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** There is no mention of validation or manual investigation of the dataset.
  - **Quote:** Not available.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not available.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Potentially, yes.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could impact the validity of results if these pairs were included in the evaluation. However, the paper does not specify the use of WT3/T4, so the direct impact is unclear.
  - **Quote:** Not available.

- **Impact on Methodology, Conclusions, or Generalizability:**
  - The methodology might be affected if the WT3/T4 subset was used, as it could lead to incorrect evaluation results. The conclusions drawn from such evaluations might be less reliable, and the generalizability of the findings could be compromised if the dataset's integrity is questioned. However, without explicit mention of WT3/T4, the extent of the impact remains speculative.

# Summary

```
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 219, No, Yes, Yes, No, Old, Yes, Not specified, No, No, No, No, Potentially
```

**Note:**  
- **G:** The paper does not specify whether the WT3/T4 subset was excluded or included.
- **L:** The potential impact of the WT3/T4 finding is speculative, as the paper does not mention this subset explicitly.
