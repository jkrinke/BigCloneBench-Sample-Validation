# Analysis of Paper 35

### Task 1: Extract Key Metadata

- **Title:** Assessing the Code Clone Detection Capability of Large Language Models
- **Authors:** Zixian Zhang, Takfarinas Saber
- **Publication Year:** 2024【4:0†source】

### Task 2: Summarize the Paper

This study evaluates the performance of two advanced Large Language Models (LLMs), GPT-3.5 and GPT-4, in detecting code clones. The research uses two datasets: BigCloneBench, which contains human-generated code clones, and GPTCloneBench, which includes LLM-generated clones. The study finds that GPT-4 consistently outperforms GPT-3.5 across all clone types, particularly in recognizing semantic nuances within code. However, both models show lower effectiveness in identifying complex Type-4 clones. The paper concludes that while GPT-4 shows promise, further improvements are needed to enhance LLM capabilities in code clone detection【4:0†source】【4:3†source】.

### Task 3: Extract Dataset Usage

The paper uses two datasets for evaluation: BigCloneBench, which is a comprehensive collection of 8 million validated clones within IJaDataset-2.0, and GPTCloneBench, a new dataset that combines GPT-engineered code clones【4:4†source】【4:6†source】.

### Task 4: Analyze BigCloneBench Usage

- **Q A:** Is the paper a systematic literature review (SLR) or a survey?
  - **A:** No.
  - **Explanation:** The paper is an empirical study evaluating the performance of LLMs in code clone detection.
  - **Quote:** "In this study, we explored the capabilities of GPT-3.5 and GPT-4 in detecting code clones"【4:15†source】.

- **Q B:** Does the paper present a novel clone detection or clone search approach or evaluate existing approaches?
  - **A:** Evaluate existing approaches.
  - **Explanation:** The paper evaluates the performance of existing LLMs, GPT-3.5 and GPT-4, in code clone detection.
  - **Quote:** "This study aims to assess the performance of two advanced Large Language Models (LLMs), GPT-3.5 and GPT-4, in the task of code clone detection"【4:0†source】.

- **Q C:** Does the paper use BigCloneBench for evaluation?
  - **A:** Yes.
  - **Explanation:** BigCloneBench is used as one of the datasets for evaluating the LLMs.
  - **Quote:** "In our study, we use two particular datasets: (i) BigCloneBench is the most used in the literature"【4:6†source】.

- **Q D:** Is BigCloneBench used as ground truth for training a machine learning approach?
  - **A:** No.
  - **Explanation:** The paper uses BigCloneBench for evaluation, not for training.
  - **Quote:** "The evaluation involves testing the models on a variety of code pairs of different clone types and levels of similarity, sourced from two datasets: BigCloneBench (human-made) and GPTCloneBench (LLM-generated)"【4:0†source】.

- **Q E:** Which version of BigCloneBench (old/new) has been used?
  - **A:** New version.
  - **Explanation:** The paper mentions using a comprehensive collection of 8 million validated clones, which corresponds to the new version.
  - **Quote:** "It is a comprehensive collection of 8 million validated clones within IJaDataset-2.0"【4:6†source】.

- **Q F:** Has the ground truth of BigCloneBench been filtered/modified? If so, what is the size of the subset used?
  - **A:** Yes, filtered.
  - **Explanation:** The paper mentions filtering out replicated clones to ensure uniqueness.
  - **Quote:** "Our initial step involves filtering out these replicated clones to ensure the uniqueness of the code clones submitted to the GPT models"【4:6†source】.

- **Q G:** Has the WT3/T4 subset been excluded from evaluation? If filtered, was WT3/T4 part of the subset?
  - **A:** No, it was included.
  - **Explanation:** The paper includes WT3/T4 in its evaluation.
  - **Quote:** "For clone Type-4, characterized by pair similarities of less than 50%, both models showed relatively low performance levels"【4:9†source】.

- **Q H:** Ignoring filtering, has the ground truth otherwise been changed, extended, or enriched?
  - **A:** No.
  - **Explanation:** The paper does not mention any changes, extensions, or enrichment to the ground truth.
  - **Quote:** Not explicitly mentioned.

- **Q I:** Has the paper used a subset created by previous work?
  - **A:** No.
  - **Explanation:** The paper does not mention using a subset created by previous work.
  - **Quote:** Not explicitly mentioned.

- **Q J:** Has the paper validated or manually investigated BigCloneBench’s ground truth?
  - **A:** No.
  - **Explanation:** The paper does not mention any validation or manual investigation of BigCloneBench’s ground truth.
  - **Quote:** Not explicitly mentioned.

- **Q K:** Does the paper cite "BigCloneBench Considered Harmful for Machine Learning"?
  - **A:** No.
  - **Explanation:** The paper does not cite this work.
  - **Quote:** Not found in the references【4:12†source】.

### Task 5: Critical Analysis – Impact of New Findings

- **Q L:** Does this finding weaken or invalidate any claims in the paper?
  - **A:** Yes.
  - **Explanation:** The finding that 93.35% of WT3/T4 pairs in BigCloneBench are not clones could significantly impact the validity of the paper's results, especially since WT3/T4 was included in the evaluation.
  - **Impact on Methodology, Conclusions, or Generalizability:** The methodology's reliance on potentially inaccurate data could lead to incorrect conclusions about the LLMs' performance. The generalizability of the findings is compromised, as the evaluation may not accurately reflect real-world clone detection capabilities. This necessitates a re-evaluation of the results using a more reliable dataset.

# Summary

```
Question, A, B, C, D, E, F, G, H, I, J, K, L
Paper 35, No, Yes, Yes, No, New, Yes, No, No, No, No, No, Yes
```

**Note:**  
- **F:** The paper filtered out replicated clones to ensure uniqueness.
- **G:** WT3/T4 was included in the evaluation, which could affect the validity of the results due to the high percentage of non-clones in this subset.
- **L:** The finding that 93.35% of WT3/T4 pairs are not clones potentially weakens the paper's claims, as it impacts the reliability of the evaluation.
